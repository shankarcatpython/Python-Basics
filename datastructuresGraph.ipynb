{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfTYnJ9KjnM95MjMZY3+HV"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "a7s8p8uledSU"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------------------------------------------\n",
        "# Data Structures and Algorithms\n",
        "# !!!!!!  Graphs !!!!!!!!\n",
        "# For Data handling We are using !!! pyspark modules !!!\n",
        "# Reason being , input data in production will be huge\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Vertices (or) points (or) Nodes\n",
        "# Edges (or) Lines (or) Links\n",
        "# Undirected Graphs\n",
        "# Directed Graphs\n",
        "# Self loops\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Paths , N nodes , N-1 Edges\n",
        "# ( N-1 Edges signifies that minimun number of edges to connect n nodes )\n",
        "# Simple path\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Undirected Graph - If every pair of Nodes has a path ( N-1 ) - No Arrow\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Directed Graph\n",
        "#   Weekly connected -->  atleast has one directed path between vertices\n",
        "#   Strongly connected --> has bidirectional paths between vertices\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SmjOV69JB7-",
        "outputId": "d75ec241-9a18-4424-d7fc-e210bafbbca3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "S5j6BjEKL5-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_list = ['Liam','Noah','Oliver','James','Elijah','William','Henry','Lucas','Benjamin',\n",
        "               'Theodore','Mateo','Levi','Sebastian','Daniel','Jack','Michael','Alexander','Owen']"
      ],
      "metadata": {
        "id": "EQWJwq5KMEL4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string_list)\n",
        "import pandas as pd\n",
        "file_dataframe = pd.DataFrame({'Names':string_list})\n",
        "file_dataframe.to_csv(r'/content/sample_data/names.csv',index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5-E7M-VMWHl",
        "outputId": "e48a6564-eabd-4085-88d8-77ae4a38cd9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Liam', 'Noah', 'Oliver', 'James', 'Elijah', 'William', 'Henry', 'Lucas', 'Benjamin', 'Theodore', 'Mateo', 'Levi', 'Sebastian', 'Daniel', 'Jack', 'Michael', 'Alexander', 'Owen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Graph\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "7Db_eETGJIWY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_dataframe = spark.read.option(\"header\",True).csv(r'/content/sample_data/names.csv')\n",
        "names_dataframe.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmHnmCB8POe3",
        "outputId": "3cc6662f-8bea-4106-c2f5-12a37ae088be"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|    Names|\n",
            "+---------+\n",
            "|     Liam|\n",
            "|     Noah|\n",
            "|   Oliver|\n",
            "|    James|\n",
            "|   Elijah|\n",
            "|  William|\n",
            "|    Henry|\n",
            "|    Lucas|\n",
            "| Benjamin|\n",
            "| Theodore|\n",
            "|    Mateo|\n",
            "|     Levi|\n",
            "|Sebastian|\n",
            "|   Daniel|\n",
            "|     Jack|\n",
            "|  Michael|\n",
            "|Alexander|\n",
            "|     Owen|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# for i in range (random.randint(1,10)):\n",
        "#    print(names_dataframe.take(random.randint(1,10)))"
      ],
      "metadata": {
        "id": "_xCIVUTLQohW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting pyspark dataframe column to a list\n",
        "\n",
        "names_list = names_dataframe.select('Names').rdd.flatMap(lambda x: x).collect()\n"
      ],
      "metadata": {
        "id": "egBMzSJHR8jK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a graph of friends in social network\n",
        "\n",
        "graph = dict()\n",
        "\n",
        "for value in names_list:\n",
        "  node = random.randint(0,9)\n",
        "  connection_set = set()\n",
        "\n",
        "  while node > 0:\n",
        "    if value != names_list[node]:\n",
        "      connection_set.add(names_list[node])\n",
        "    node-=1\n",
        "\n",
        "    temp_list = list(connection_set)\n",
        "    random.shuffle(temp_list)\n",
        "\n",
        "    graph[value] = temp_list\n",
        "\n",
        "for key, value in graph.items():\n",
        "  print('[',key,']' , ' --> ',value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2fi6rG-iDCy",
        "outputId": "35efc44a-1634-4b45-cb57-582158acec71"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Liam ]  -->  ['Elijah', 'James', 'Noah', 'Lucas', 'William', 'Benjamin', 'Oliver', 'Henry']\n",
            "[ Noah ]  -->  ['Elijah', 'James', 'William', 'Oliver']\n",
            "[ James ]  -->  ['Noah', 'Oliver']\n",
            "[ Elijah ]  -->  ['Noah', 'Oliver', 'James', 'William']\n",
            "[ William ]  -->  ['Henry', 'Oliver', 'Noah', 'Elijah', 'James']\n",
            "[ Henry ]  -->  ['Noah', 'Oliver', 'James']\n",
            "[ Lucas ]  -->  ['Noah']\n",
            "[ Benjamin ]  -->  ['Oliver', 'James', 'Noah', 'Elijah']\n",
            "[ Levi ]  -->  ['Noah', 'Oliver']\n",
            "[ Sebastian ]  -->  ['James', 'Oliver', 'Henry', 'Elijah', 'William', 'Noah']\n",
            "[ Daniel ]  -->  ['Elijah', 'James', 'Noah', 'Oliver']\n",
            "[ Jack ]  -->  ['James', 'Noah', 'Oliver', 'Elijah']\n",
            "[ Michael ]  -->  ['Noah', 'Oliver']\n",
            "[ Alexander ]  -->  ['Elijah', 'James', 'Oliver', 'Noah']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: stop spark session\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "id": "gNvu1bzebqnp"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}